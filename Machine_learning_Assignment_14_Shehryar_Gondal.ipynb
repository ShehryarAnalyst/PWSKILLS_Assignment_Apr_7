{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b88351e",
   "metadata": {},
   "source": [
    "## Assignment Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5602b149",
   "metadata": {},
   "source": [
    "__Q1. What is the relationship between polynomial functions and kernel functions in machine learning\n",
    "algorithms?__\n",
    "\n",
    "__Ans)__ In machine learning algorithms, kernel functions are often used to implicitly represent higher-dimensional feature spaces without explicitly computing the transformations. Polynomial functions are one type of kernel function commonly used in machine learning algorithms.\n",
    "\n",
    "The relationship between polynomial functions and kernel functions lies in the fact that polynomial functions can be used as a kernel function to transform the input features into a higher-dimensional space. The kernel function computes the inner product between two transformed feature vectors in this higher-dimensional space.\n",
    "\n",
    "Polynomial kernel functions are defined as K(x, y) = (x • y + c)^d, where x and y are input feature vectors, c is a constant, and d is the degree of the polynomial. The kernel function computes the dot product between the transformed feature vectors (x • y) in the original feature space, raised to the power of the polynomial degree (d). By using this kernel function, the data is implicitly transformed into a higher-dimensional space where it becomes easier to find linearly separable boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e816555",
   "metadata": {},
   "source": [
    "__Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fed4d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset (e.g., Iris dataset)\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an SVM classifier with a polynomial kernel\n",
    "svm = SVC(kernel='poly', degree=3)  # Specify the kernel type and degree\n",
    "\n",
    "# Train the SVM classifier\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b398a129",
   "metadata": {},
   "source": [
    "__Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?__\n",
    "\n",
    "__Ans)__ In Support Vector Regression (SVR), epsilon (ε) is a parameter that controls the width of the margin around the regression line. It determines the tolerance for errors in the training data. Specifically, epsilon defines a tube around the regression line within which errors are considered acceptable and do not contribute to the loss function.\n",
    "\n",
    "Increasing the value of epsilon in SVR typically leads to an increase in the number of support vectors. Support vectors are the data points that lie within or on the margin boundaries or violate the margin boundaries.\n",
    "\n",
    "Here's how increasing the value of epsilon affects the number of support vectors in SVR:\n",
    "\n",
    "Smaller Epsilon (ε): When epsilon is set to a small value, the margin around the regression line becomes narrow, and SVR becomes more sensitive to errors. This leads to a smaller number of support vectors as the model tries to fit the data as closely as possible, even if it means including more data points within the margin boundaries.\n",
    "\n",
    "Larger Epsilon (ε): Increasing the value of epsilon widens the margin around the regression line, allowing for more errors to be tolerated. This results in a larger number of support vectors as the model becomes more relaxed and allows more data points to fall within the wider margin boundaries.\n",
    "\n",
    "It's important to note that the number of support vectors can also be influenced by other factors, such as the complexity of the data, the choice of kernel function, and the regularization parameter. However, in general, increasing epsilon in SVR tends to lead to an increase in the number of support vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0eaed9",
   "metadata": {},
   "source": [
    "__Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter\n",
    "affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works\n",
    "and provide examples of when you might want to increase or decrease its value?__\n",
    "\n",
    "__Ans)__ \n",
    "\n",
    "1. Kernel Function: The choice of kernel function determines the type of non-linear mapping used to capture relationships in the data. Different kernels (linear, polynomial, Gaussian) have distinct properties for handling different data patterns.\n",
    "\n",
    "2. C Parameter: The C parameter controls the trade-off between training error and model complexity. Smaller C values increase regularization, preventing overfitting, while larger values reduce regularization, potentially improving training performance.\n",
    "\n",
    "3. Epsilon Parameter: Epsilon defines the acceptable error margin in SVR. Smaller values result in a precise fit with less tolerance for errors, while larger values provide a smoother fit with greater error tolerance.\n",
    "\n",
    "4. Gamma Parameter: Gamma is specific to the Gaussian/RBF kernel and controls the width of the curve. Smaller gamma values yield smoother decision boundaries, while larger values capture more intricate details and can lead to overfitting.\n",
    "\n",
    "5. Adjusting these parameters depends on the dataset characteristics, noise level, and desired trade-off between accuracy and generalization. Hyperparameter tuning techniques are recommended to find optimal values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb87a20",
   "metadata": {},
   "source": [
    "Q5. Assignment:\n",
    "- Import the necessary libraries and load the dataseg\n",
    "-  Split the dataset into training and testing setZ\n",
    "- Preprocess the data using any technique of your choice (e.g. scaling, normaliMationK\n",
    "- Create an instance of the SVC classifier and train it on the training datW\n",
    "- hse the trained classifier to predict the labels of the testing datW\n",
    "- Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy, precision, recall, F1-scoreK\n",
    "- Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomiMedSearchCV to improve its performanc_\n",
    "-  Train the tuned classifier on the entire dataseg\n",
    "- Save the trained classifier to a file for future use.\n",
    "\n",
    "__You can use any dataset of your choice for this assignment, but make sure it is suitable for\n",
    "classification and has a sufficient number of features and samples.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a18f1bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['svm_classifier.pkl']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "\n",
    "# Load the dataset (Iris dataset)\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the data by scaling (StandardScaler)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create an instance of SVC classifier\n",
    "svc = SVC()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svc.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict the labels of the testing data\n",
    "y_pred = svc.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the performance of the classifier (accuracy score)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Tune hyperparameters using GridSearchCV\n",
    "param_grid = {'C': [0.1, 1, 10], 'gamma': [0.1, 0.01, 0.001]}\n",
    "grid_search = GridSearchCV(svc, param_grid, cv=5)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Train the tuned classifier on the entire dataset\n",
    "svc_tuned = grid_search.best_estimator_\n",
    "svc_tuned.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Save the trained classifier to a file\n",
    "joblib.dump(svc_tuned, 'svm_classifier.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
